1. 确保所有主题存在
./kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic data
./kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic alarm
./kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic vbData

2.常见错误
>> NoNodeAvailableException[None of the configured nodes are available: 
	ES 错误
>>spark 54:39 INFO yarn.Client: Application report for application_1505131453769_0004 (state: ACCEPTED)
	内存不足，任务提交失败
	查看Slave上的nodemanager是否启动(查看resource manager日志)  tail -n 1000 ./hadoop-2.7.4/logs/yarn-anxin-resourcemanager-node35.log

>> GOPATH entry is relative; must be absolute path: "".
	系统路径后的分号去掉

>>  When running with master 'yarn-cluster' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment
	SPark-env.sh中增加 HADOOP_CONF_DIR or YARN_CONF_DIR

>> Required executor memory (1024+384 MB) is above the max threshold (1024 MB) of this cluster! Please check the values of 'yarn.scheduler.maximum-allocation-mb' and/or 'yarn.nodemanager.resource.memory-mb'.
	hadoop中yarn-site.xml 中修改yarn.nodemanager.resource.memory-mb 为4096

>>  org.apache.hadoop.security.AccessControlException: Permission denied: user=root, access=WRITE, inode="/":anxin:supergroup:drwxr-xr-x
	在hdfs-site.xml中
	<pre name="code" class="html"><span style="font-size:18px;"> <property>  
		<name>dfs.permissions</name>  
		<value>false</value>  
	</property></span>
  

>> yarn.Client: Application report for application_1505465474108_0001 (state: ACCEPTED)  一直提示这条信息，表示spark接受了请求的job，但集群式没有足够的资源来运行它
	http://node35:8088/cluster/apps  yarn cluster manager
	yarn application -kill application_1505466266369_0004 杀掉其他应用 (未成功)
	
>> 日志区分(保留进程Info日志,去除)
	Spark/conf 目录 log4j
	log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=WARN
	log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=WARN
	log4j.logger.org.apache.spark=WARN (++important)

>> [WARN]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
	可以忽略； 否则重新编译   http://www.ercoppa.org/posts/how-to-compile-apache-hadoop-on-ubuntu-linux.html


3. 启动http-server

	http_server.exe -addr 10.8.30.117:8082 -brokers 127.0.0.1:9092
	
	-addr 监听地址(和iota规则引擎配置对应)
	-brokers kafka代理地址
	
4. 环境环境
		>> Linux 操纵
			sudo apt install openjdk-8-jre
			top 资源管理器
			vmstat 10 
			free -m  查看内存使用情况 -m M -g G
			/etc/profile 
					export JAVA_HOME=/usr
					export CLASSPATH=.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$CLASSPATH
					export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH
					export HADOOP_HOME=/home/yww/tes/hadoop-2.7.4
					export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
			source /etc/profile
			执行权限 chmod -R +x ../bin
			关闭防火墙 sudo ufw disable
			jps 查看后台进程
			ps -ef | grep pname     kill -pid pid/ppid (kill -TERM PPID)    killall pname     kill -HUP pid 进程重启    kill -9 PID 强制
		
		hdfs namenode -format
		hdfs dfs -mkdir /test
		sbin/start-all.sh
			or sbin/start-dfs  sbin/start-yarn (***如果使用独立的spark集群提交et，无需启动yarn)
		
		./elasticsearch -d
		ps aux|grep elasticsearch
		
		./zkServer.sh start
		nohup ./kafka-server-start.sh ../config/server.properties 1>/dev/null 2>&1 &
		
		spark/sbin/start-all.sh
			or /sbin"/start-master.sh  /sbin"/start-slaves.sh
			
			
		nvm和nodejs
		wget -qO- https://raw.githubusercontent.com/creationix/nvm/v0.33.4/install.sh | bash
			export NVM_DIR="$HOME/.nvm"
			[ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"  # This loads nvm
			[ -s "$NVM_DIR/bash_completion" ] && \. "$NVM_DIR/bash_completion"  # This loads nvm bash_completion
		nvm install 8.4.0
		npm install -g pm2
		
		
		[1] Spark on Yarn 环境搭建 [http://www.jianshu.com/p/b1ac056f8a3a]
		[2] Hadoop集群环境搭建（三台） [http://www.jianshu.com/p/bb7ce32a1920]
		[3] hadoop分布式集群部署步骤总结 [http://blog.csdn.net/u010330043/article/details/51235373]
		* 关闭 ES -> kill -9 22158
		
5. spark提交的几种方式	[http://www.th7.cn/system/lin/201705/214258.shtml]
	1. 单机模式(Standalone)
	./bin/spark-submit  --class   org.apache.spark.examples.SparkPi  --master   local  examples/jars/spark-examples_2.11-2.1.1.jar
	2. 使用独立的Spark集群模式提交任务
	3. 使用spark+Hadoop集群
		3.1 yarn-client模式执行
			./bin/spark-submit   --class  org.apache.spark.examples.SparkPi  --master  yarn-client    examples/jars/spark-examples_2.11-2.1.1.jar
		3.2 yarn-cluster模式执行
			When running with master 'yarn-client' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment
		
		[Spark:Yarn-cluster和Yarn-client区别与联系][https://www.iteblog.com/archives/1223.html]

..,,info:
	tracking URL: http://dragon-Q:8088/proxy/application_1505118534669_0001/
	change host 10.8.30.199 -> dragon-Q
	
	查看yarn日志 yarn logs -applicationId 
	日志聚集 yarn.log-aggregation-enable
	日志在hadoop安装目录下(yarn.nodemanager.log-dirs) /home/yww/tes/hadoop-2.7.4/logs/userlogs/application_1505118534669_0001
	
	spark-submit从conf/spark-defaults.conf读取配置
	
	1) 失败
./spark-2.2.0-bin-hadoop2.7/bin/spark-submit --master yarn-cluster ./et/et-1.0-SNAPSHOT-jar-with-dependencies.jar 
	2) 失败
./spark-2.2.0-bin-hadoop2.7/bin/spark-submit --master yarn --deploy-mode cluster --driver-memory 1g --executor-memory 1g ./et/et-1.0-SNAPSHOT-jar-with-dependencies.jar 
	3) 失败
./spark-2.2.0-bin-hadoop2.7/bin/spark-submit --class anxin.et.main --master yarn --deploy-mode cluster --driver-memory 1g --executor-memory 1g hdfs://10.8.30.35:9000/user/anxin/code/et/	et-1.0-SNAPSHOT-jar-with-dependencies.jar
	4) 失败
	
>> 提交到spark集群
	5) 成功(client模式)
./spark-2.2.0-bin-hadoop2.7/bin/spark-submit --master spark://10.8.30.35:7077 ./et/et-1.0-SNAPSHOT-jar-with-dependencies.jar 
	6) 成功 (so...)
./spark-2.2.0-bin-hadoop2.7/bin/spark-submit --class anxin.et.main --master spark://10.8.30.35:6066 --deploy-mode cluster hdfs://10.8.30.35:9000/user/anxin/code/et/et-1.0-SNAPSHOT-jar-with-dependencies.jar
	7) 成功 (with zookeeper HA)
./spark-2.2.0-bin-hadoop2.7/bin/spark-submit --class anxin.et.main --master spark://node35:6066,node36:6066,node37:6066 --deploy-mode cluster hdfs://node35:9000/user/anxin/code/et.jar
	
	*提交文件到HDFS
	 hdfs dfs -rm hdfs://node35:9000/user/anxin/code/et.jar
	 hdfs dfs -put ./et.jar hdfs://node35:9000/user/anxin/code/et.jar
	
6. 控制台(Default Port)
	spark管理界面 [http://10.8.30.35:8080/]
		work node 36 [http://10.8.30.36:8081]
		work node 37 [http://10.8.30.37:8081]
	
			URL: spark://node35:7077
			REST URL: spark://node35:6066 (cluster mode)
			
	HDFS http://node35:50070/
	
	HADOOP yarn (暂未用)
		http://node35:8088
	
6.extra -- HA
	master存在单节点错误 
	实现基于zookeeper的HA
	spark-env.sh中添加
		export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node35:2181,node36:2181,node37:2181 -Dspark.deploy.zookeeper.dir=/spark"
	scp ./spark-2.2.0-bin-hadoop2.7/conf/spark-env.sh anxin@node36:/home/anxin/spark-2.2.0-bin-hadoop2.7/conf
	scp ./spark-2.2.0-bin-hadoop2.7/conf/spark-env.sh anxin@node37:/home/anxin/spark-2.2.0-bin-hadoop2.7/conf
		
		anxin@node35:~$ ./spark-2.2.0-bin-hadoop2.7/sbin/start-all.sh
		anxin@node36:~$ ./spark-2.2.0-bin-hadoop2.7/sbin/start-master.sh
		anxin@node37:~$ ./spark-2.2.0-bin-hadoop2.7/sbin/start-master.sh

		试验：
		http://node35:8080/  Status: ALIVE
		http://node36:8080/  Status: STANDBY
		http://node37:8080/  Status: STANDBY
		关闭node35上master
		anxin@node35:~$ ./spark-2.2.0-bin-hadoop2.7/sbin/stop-master.sh
		http://node35:8080/  Cannot access
		http://node36:8080/  Status: STANDBY
		http://node37:8080/  Status: ALIVE
		
	
7. 附录	
	
<start-all.sh>

### start et in 10.8.30.99, script work dir is /home/yww/tes
### 20170911
echo <zookeeper>
./zookeeper-3.4.10/bin/zkServer.sh start

echo <kafka>
nohup ./kafka_2.11-0.11.0.0/bin/kafka-server-start.sh ./kafka_2.11-0.11.0.0/config/server.properties 1>/dev/null 2>&1 &
# ./kafka_2.11-0.11.0.0/bin/kafka-server-start.sh  -daemon  ./kafka_2.11-0.11.0.0/config/server.propertie
## ssh-keygen -t rsa -P ""
## cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys
ssh localhost
cd tes
echo <hadoop>
./hadoop-2.7.4/sbin/start-all.sh

echo <ES>
./elasticsearch-5.5.2/bin/elasticsearch start -d

echo <ET>
# not use anymore
./spark-2.1.1/bin/spark-submit --master yarn-cluster ./et/et-1.0-SNAPSHOT-jar-with-dependencies.jar 




log4j配置
$SPARK_HOME/conf/log4j.properties
# Set everything to be logged to the console
log4j.rootCategory=INFO, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} [ %t:%r ] %p %c{1}: %m%n

# Set the default spark-shell log level to WARN. When running the spark-shell, the
# log level for this class is used to overwrite the root logger's log level, so that
# the user can have different defaults for the shell and regular Spark apps.
log4j.logger.org.apache.spark.repl.Main=WARN

# Settings to quiet third party logs that are too verbose
log4j.logger.org.spark_project.jetty=WARN
log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERROR
log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=WARN
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=WARN
log4j.logger.org.apache.parquet=ERROR
log4j.logger.parquet=ERROR
log4j.logger.org.apache.spark=WARN
log4j.logger.org.apache.zookeeper=WARN
log4j.logger.kafka=WARN
log4j.logger.org.I0Itec.zkclient=WARN

# SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support
log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL
log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR



调试条件“
I:\WorkSpace\@JAVA\zookeeper-3.4.10\bin\zkServer.cmd

cd /d I:\WorkSpace\@JAVA\kafka_2.11-0.11.0.0
RD /S /Q I:\tmp\kafka-logs 
.\bin\windows\kafka-server-start.bat .\config\server.properties

cd /d I:\FS-SavoirCloud\trunk\codes\services\iota-proxy
npm start

cd /d I:\FS-SavoirCloud\trunk\codes\services\go\src\anxin\recv
http_server.exe -addr 10.8.30.117:8073 -brokers node35:9092,node36:9092,node37:9092

cd /d I:\FS-SavoirCloud\trunk\codes\services\go\src\anxin\recv
http_server.exe -addr 10.8.30.117:8071 -brokers localhost:9092

kibana
http://10.8.30.37:5601
Iota
https://console.theiota.cn/
iota Doc
https://doc.theiota.cn
spark ui
http://10.8.30.35:8080/
Savoir
http://10.8.30.35:8081/signin
test  tester 123456



spark-submit --class anxin.et.main --master spark://anxin-m1:6066,anxin-m2:6066 --deploy-mode cluster hdfs://iota-m1:8020/savoir/code/et.jar