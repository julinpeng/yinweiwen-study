以下内容摘自 https://www.sohu.com/a/403477409_411876

1. Databricks 和 Delta

2. Uber 和 Apache Hudi

3. Netflix和Apache Iceberg

数据湖痛点

![img](imgs/数据湖实践/f0aabfec7f5f46c794bf7995ff9fbc72.jpeg)



## Apache Iceberg

**Apache Iceberg is an open table format for huge analytic datasets.** 





### 安装HIVE

https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-InstallationandConfiguration

参考[博文](https://blog.csdn.net/weixin_43861175/article/details/90372513)

Step by step >>>

在157测试环境

![image-20210813114749531](imgs/数据湖实践/image-20210813114749531.png)

环境准备：

> '>java 1.7 
>
> Hadoop 3.1  (HADOOP_HOME)



[Download hive](http://www.apache.org/dyn/closer.cgi/hive/)  [【3.1.2】](https://ftp.acc.umu.se/mirror/apache.org/hive/hive-3.1.2/)



```shell
$ tar -xzvf hive-x.y.z.tar.gz
$ cd hive-x.y.z
$ export HIVE_HOME={{pwd}}
$ export PATH=$HIVE_HOME/bin:$PATH

export HADOOP_HOME=${HADOOP_HOME:-/usr/hdp/3.1.4.0-315/hadoop}

 $ $HADOOP_HOME/bin/hadoop fs -mkdir       /tmp
 $ $HADOOP_HOME/bin/hadoop fs -mkdir       /user/hive/warehouse
 $ $HADOOP_HOME/bin/hadoop fs -chmod g+w   /tmp
 $ $HADOOP_HOME/bin/hadoop fs -chmod g+w   /user/hive/warehouse
 
 # run HIVE CLI
 $HIVE_HOME/bin/hive
 
 vi /etc/profile
 """
 export HIVE_HOME=/home/anxin/apache-hive-3.1.2-bin
 export PATH=$HIVE_HOME/bin:$PATH
 """
 hive --version
 
 #启动报错 com.google.common.base.Preconditions.checkArgument
#: guava包冲突
rm lib/guava-19.0.jar
cp /usr/hdp/3.1.4.0-315/hadoop/lib/guava-28.0-jre.jar /home/anxin/apache-hive-3.1.2-bin/lib/

# 修改配置
mv hive-default.xml.template hive-site.xml

```

hive-site.xml 中添加数据库连接属性

通过vi命令 输入 `/abc`进行查找 `N`查找下一条

配置文件里有一行报错，输入 `:n`跳转到指定第n行。

```xml
<?xml version="1.0" encoding="UTF-8" standalone="no"?><?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>123456</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://10.8.30.157:3305/metastore_db?createDatabaseIfNotExist=true</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
    </property>
    <property>
        <name>hive.metastore.schema.verification</name>
        <value>false</value>
    </property>
    <property>
        <name>hive.cli.print.current.db</name>
        <value>true</value>
    </property>
    <property>
        <name>hive.cli.print.header</name>
        <value>true</value>
    </property>

    <!-- hiveserver2 -->
    <property>
        <name>hive.server2.thrift.port</name>
        <value>10000</value>
    </property>
    <property>
        <name>hive.server2.thrift.bind.host</name>
        <value>10.8.30.37</value>
    </property>
</configuration>
```

下载mysql驱动

https://downloads.mysql.com/archives/c-j/

```shell
sudo dpkg -i mysql-connector-java_8.0.25-1ubuntu16.04_all.deb
cp /usr/share/java/mysql-connector-java-8.0.25.jar $HIVE_HOME/lib
```



`Exception in thread "main" java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: ${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D`

![img](imgs/数据湖实践/20181229154259580.png)



```shell
$HIVE_HOME/bin/schematool -dbType mysql -initSchema

$HIVE_HOME/bin/hiveserver2

root@node37:/home/anxin/apache-hive-3.1.2-bin# hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/anxin/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/3.1.4.0-315/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = f02a15a1-7faa-4a4f-bf50-d1889082a36f

Logging initialized using configuration in jar:file:/home/anxin/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Hive Session ID = ed686748-4415-4c87-a7ab-c3790728c161
hive> create database hive_1;
OK
Time taken: 0.96 seconds
hive> show databases;
OK
default
hive_1
Time taken: 1.343 seconds, Fetched: 2 row(s)
```





HiveServer2

HiveServer2（HS2）是一个服务端接口，使远程客户端可以执行对Hive的查询并返回结果。目前基于Thrift RPC的实现是HiveServer的改进版本，并支持多客户端并发和身份验证.

Beeline 是HiveServer2自己的CLI。启动beeline：

```shell
$HIVE_HOME/bin/beeline -u  jdbc:hive2://10.8.30.37:10000
```

启动报错：

```shell
Connecting to jdbc:hive2://node37:10000
21/08/16 14:54:17 WARN jdbc.HiveConnection: Failed to connect to node37:10000
Error: Could not open client transport with JDBC Uri: jdbc:hive2://node37:10000: Failed to open new session: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): Unauthorized connection for super-user: root from IP 10.8.30.37 (state=08S01,code=0)
Beeline version 3.1.2 by Apache Hive
```

授权失败，修改hdfs的proxy.user权限（通过ambari）

![image-20210816155402568](imgs/数据湖实践/image-20210816155402568.png)

通过10002端口访问

![image-20210816160303981](imgs/数据湖实践/image-20210816160303981.png)





同时启动hiveserver2和beeline测试：

```shell
$HIVE_HOME/bin/beeline -u  jdbc:hive2://
Connected to: Apache Hive (version 3.1.2)
Driver: Hive JDBC (version 3.1.2)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 3.1.2 by Apache Hive
0: jdbc:hive2://> 
0: jdbc:hive2://> show databases;
OK
+----------------+
| database_name  |
+----------------+
| default        |
| hive_1         |
+----------------+
2 rows selected (2.095 seconds)
0: jdbc:hive2://> CREATE TABLE pokes (foo INT, bar STRING);
OK
No rows affected (1.59 seconds)
0: jdbc:hive2://> show tables;
OK
+-----------+
| tab_name  |
+-----------+
| pokes     |
+-----------+
1 row selected (0.215 seconds)
```



![img](https://upload-images.jianshu.io/upload_images/15282588-09d1c147ed10abb6.png?imageMogr2/auto-orient/strip|imageView2/2/w/476/format/webp)

hiveserver的访问方式

  1   hive 命令行模式，直接输入/hive/bin/hive的[执行程序](https://www.baidu.com/s?wd=执行程序&tn=44039180_cpr&fenlei=mv6quAkxTZn0IZRqIHckPjm4nH00T1Y3rywWPHTsujTYnjDdPH9b0ZwV5Hcvrjm3rH6sPfKWUMw85HfYnjn4nH6sgvPsT6KdThsqpZwYTjCEQLGCpyw9Uz4Bmy-bIi4WUvYETgN-TLwGUv3ErHnLPjbLn1m1P1b4rHbsnHTz)，或者输入 hive --service cli

  2  hive web界面的 (端口号9999) 启动方式 hive –service hwi & 用于通过浏览器来访问hive，感觉没多大用途

 3  hive 远程服务 (端口号10000) 启动方式

   hive --service hiveserver & 或者 hive --service hiveserver 10000>/dev/null 2>/dev/null &

  beeline方式连接：beeline -u jdbc:hive2//localhost:10000/default -n root -p 123456  或者java client方式连接

  备注：连接Hive JDBC URL：jdbc:hive://192.168.6.116:10000/default   （Hive默认端口：10000; 默认数据库名：default）



作者：jero_lei
链接：https://www.jianshu.com/p/45ca74ec1e3f
来源：简书
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。



### Flink中使用IceBerg

[参考] https://www.cxyzjd.com/article/xuronghao/109764018

[官网] https://iceberg.apache.org/flink/

下载flink 1.11 建议scala 2.12

https://flink.apache.org/downloads.html

设置HADOOP_HOME以及HADOOP_CLASSPATH: `/etc/profile`

```shell
export HADOOP_HOME=/usr/hdp/3.1.4.0-315/hadoop
export HADOOP_CLASSPATH=`$HADOOP_HOME/bin/hadoop classpath`
```



启动基于hadoop环境的standalone模式：

```shell
./bin/start-cluster.sh
```

启动flink sql client

```shell
./bin/sql-client.sh embedded \
    -j /home/anxin/iceberg/iceberg-flink-runtime-0.12.0.jar \
    -j /home/anxin/iceberg/flink-sql-connector-hive-2.3.6_2.11-1.11.0.jar \
    shell
```

 <img src="imgs/数据湖实践/image-20210816134623372.png" width="500" align="lfet">



创建catalog

```sql
CREATE CATALOG hive_catalog WITH (
  'type'='iceberg',
  'catalog-type'='hive',
  'uri'='thrift://10.8.30.37:10000',
  'clients'='5',
  'property-version'='1',
  'warehouse'='hdfs://node37:8020/user/hive/warehouse'
);
```

创建Hive Catalog一直失败，尝试另外一种方法： https://jiamaoxiang.top/2020/12/21/Flink%E9%9B%86%E6%88%90Hive%E4%B9%8BHive-Catalog%E4%B8%8EHive-Dialect-%E4%BB%A5Flink1-12%E4%B8%BA%E4%BE%8B/#Flink-SQLCli%E4%B8%AD%E4%BD%BF%E7%94%A8Hive-Catalog

修改Flink SQL中配置文件 `conf/sql-client-defaults.yaml`

```yaml
#==============================================================================
# Catalogs
#==============================================================================

# Define catalogs here.
catalogs:
  - name: myhive
    type: hive
    default-database: default
    hive-conf-dir: /home/anxin/apache-hive-3.1.2-bin/conf
```

flink启动sql-client.sh embedded报错 `Make sure you have set a valid value for hive.metastore.uris`

配置hive-site.xml

```xml
<property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
</property>

<property>
        <name>hive.metastore.local</name>
        <value>false</value>
</property>

<property>
        <name>hive.metastore.uris</name>
        <value>thrift://10.8.30.37:9083</value>
</property>
```

执行sql-client.sh embedded继续报错：

```shell
2021-08-17 15:22:17,994 WARN  org.apache.hadoop.hive.metastore.RetryingMetaStoreClient     [] - MetaStoreClient lost connection. Attempting to reconnect (1 of 1) after 1s. getDatabase
```

原因是我们没有启动HSM：

```shell
root@node37:/home/anxin/apache-hive-3.1.2-bin/conf# hive --service metastore
2021-08-17 15:27:28: Starting Hive Metastore Server
```

再执行sql-client.sh embedded成功：

![image-20210817153538148](imgs/数据湖实践/image-20210817153538148.png)





创建一张连接kafka的表

```sql
CREATE TABLE anxinyun_data ( 
    `userId` STRING, -- 用户id
    `thingId` STRING,
    `deviceId` STRING,
    `taskId` STRING,
    `dimensionId` STRING,
    -- `nw` AS PROCTIME(), -- 通过计算列产生一个处理时间列
    `triggerTime` DATE
    -- `triggerTime` AS TO_TIMESTAMP(FROM_UNIXTIME(ts, 'yyyy-MM-ddTHH:mm:ss.SSS')) -- 事件时间
 ) WITH ( 
    'connector' = 'kafka-0.10', -- 使用 kafka connector
    'topic' = 'anxinyun_data', -- kafka主题
    'scan.startup.mode' = 'earliest-offset', -- 偏移量
    'properties.group.id' = 'flink.sql', -- 消费者组
    'properties.bootstrap.servers' = '10.8.30.37:6667,10.8.30.38:6667,10.8.30.156:6667', 
    'format' = 'json', -- 数据源格式为json
    'json.fail-on-missing-field' = 'false',
    'json.ignore-parse-errors' = 'false',
    'is_generic' = 'false' -- 创建HIVE兼容表
);
```

需要下载kafka-connect包到flink-sql的classpath下：

`flink-sql-connector-kafka-0.11_2.11-1.11.4.jar`



在Hive中查看该表的元数据：

```shell
hive (default)> desc formatted anxinyun_data;
OK
col_name	data_type	comment
# col_name            	data_type           	comment             
	 	 
# Detailed Table Information	 	 
Database:           	default             	 
OwnerType:          	USER                	 
Owner:              	null                	 
CreateTime:         	Tue Aug 17 15:48:22 CST 2021	 
LastAccessTime:     	UNKNOWN             	 
Retention:          	0                   	 
Location:           	hdfs://node37:8020/user/hive/warehouse/anxinyun_data	 
Table Type:         	MANAGED_TABLE       	 
Table Parameters:	 	 
	flink.connector     	kafka-0.10          
	flink.format        	json                
	flink.json.fail-on-missing-field	false               
	flink.json.ignore-parse-errors	false               
	flink.properties.bootstrap.servers	10.8.30.37:6667,10.8.30.38:6667,10.8.30.156:6667
	flink.properties.group.id	flink.sql           
	flink.scan.startup.mode	earliest-offset     
	flink.schema.0.data-type	VARCHAR(2147483647) 
	flink.schema.0.name 	userId              
	flink.schema.1.data-type	VARCHAR(2147483647) 
	flink.schema.1.name 	thingId             
	flink.schema.2.data-type	VARCHAR(2147483647) 
	flink.schema.2.name 	deviceId            
	flink.schema.3.data-type	VARCHAR(2147483647) 
	flink.schema.3.name 	taskId              
	flink.schema.4.data-type	VARCHAR(2147483647) 
	flink.schema.4.name 	dimensionId         
	flink.schema.5.data-type	TIMESTAMP(3) NOT NULL
	flink.schema.5.expr 	PROCTIME()          
	flink.schema.5.name 	nw                  
	flink.schema.6.data-type	DATE                
	flink.schema.6.name 	triggerTime         
	flink.topic         	anxinyun_data4      
	**********
	is_generic          	true
	**********
	transient_lastDdlTime	1629186502          
	 	 
# Storage Information	 	 
SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
OutputFormat:       	org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat	 
Compressed:         	No                  	 
Num Buckets:        	-1                  	 
Bucket Columns:     	[]                  	 
Sort Columns:       	[]                  	 
Storage Desc Params:	 	 
	serialization.format	1                   
Time taken: 1.764 seconds, Fetched: 48 row(s)
hive (default)> 

```



执行select时候出错：

Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.security.Credentials

~·· TODO

















































### Flink Stateful Functions

https://www.infoq.cn/article/0xbsdhidulnllpph8vo0

Apache Flink流处理 + FaaS函数即服务

 A Platform-Independent Stateful Serverless Stack



Stateful Functions is an API that simplifies the building of **distributed stateful applications** with a runtime built for **serverless architectures**. It brings together the benefits of stateful stream processing - the processing of large datasets with low latency and bounded resource constraints - along with a runtime for modeling stateful entities that supports location transparency, concurrency, scaling, and resiliency.





![Stateful Functions](imgs/数据湖实践/arch_overview.svg)

![img](imgs/数据湖实践/model-score.svg)



支持 Stateful Functions 的运行时是基于 Apache Flink 流处理的。状态保存在流处理引擎中，与计算位于同一位置，并能提供快速且一致的状态访问。状态的持久性和容错性是建立在 Flink 具有鲁棒性的[分布快照模型](https://ci.apache.org/projects/flink/flink-docs-stable/internals/stream_checkpointing.html)上的。



### Spring Cloud

Spring Cloud 介绍

https://zhuanlan.zhihu.com/p/95696180?from_voters_page=true

服务发现(Eureka) + 负载均衡(Ribbon)